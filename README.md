# Papers about Neural Backdoor
A list of papers of Neural Backdoor

# Neural Trojan Attacks

## Training Data Poisoning
* Neural network trojan (Journal of Computer Security, 2013) [[Paper](https://content.iospress.com/articles/journal-of-computer-security/jcs460)]

* Backdoor attacks against learning systems (CNS, 2017) [[paper](https://ieeexplore.ieee.org/document/8228656)]

* Trojaning attack on neural networks (NDSS, 2018) [[paper](https://www.cs.purdue.edu/homes/ma229/papers/NDSS18.TNN.pdf)]

* Neural trojans (ICCD, 2017) [[paper](https://arxiv.org/pdf/1710.00942.pdf)]

* Design of intentional backdoors in sequential models (2019) [[paper](https://arxiv.org/pdf/1902.09972.pdf)]

* Targeted backdoor attacks on deep learning systems using data poisoning (2017) [[paper](https://arxiv.org/pdf/1712.05526.pdf)]

### Hiding Trojan Triggers

* A new Backdoor Attack in CNNs by training set corruption without label poisoning (2019) [[paper](https://arxiv.org/pdf/1902.11237.pdf)]

* Backdoor embedding in convolutional neural network models via invisible perturbation (2018) [[paper](https://arxiv.org/pdf/1808.10307.pdf)]

* Invisible Backdoor Attacks Against Deep Neural Networks (2019) [[paper](https://arxiv.org/pdf/1909.02742.pdf)]

* Hidden Trigger Backdoor Attacks (2019) [[paper](https://arxiv.org/pdf/1910.00033.pdf)]

## Altering Training Algorithms

* Backdoor Attacks on Neural Network Operations (GlobalSIP 2018) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8646335)]


  
### Trojan Insertion via Transfer Learning

* BadNets: Evaluating Backdooring Attacks on Deep Neural Networks (IEEE access 2019) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8685687)]

* Latent Backdoor Attacks on Deep Neural Networks (CCS 19) [[paper](http://people.cs.uchicago.edu/~huiyingli/publication/fr292-yaoA.pdf)]


### Neural Trojans in Hardware

* Hu-fu: Hardware and software collaborative attack framework against neural networks (ISVLSI 2018) [[paper](https://arxiv.org/abs/1805.05098)]

* Hardware trojan attacks on neural networks (2018) [[paper](https://arxiv.org/pdf/1806.05768.pdf)]

## Binary-Level Attacks

* SIN 2: Stealth infection on neural networkâ€”a low-cost agile neural trojan attack methodology. (HOST 2018) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8383920)]



# Defense Techniques

## Neural Network Verification

* Quantitative Verification of Neural Networks And its Security Applications (2019) [[paper](https://arxiv.org/pdf/1906.10395.pdf)]

* Sensitive-Sample Fingerprinting of Deep Neural Networks (CVPR 2019) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_Sensitive-Sample_Fingerprinting_of_Deep_Neural_Networks_CVPR_2019_paper.pdf)]

## Trojan Trigger Detection

* Detecting Poisoning Attacks on Machine Learning in IoT Environments (ICIOT 2018) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8473440)]

* Debugging Machine Learning Tasks (2016) [[paper](https://arxiv.org/pdf/1603.07292.pdf)]

* DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep
Neural Networks (IJCAI 2019) [[paper](https://www.ijcai.org/Proceedings/2019/0647.pdf)]

* STRIP: A Defence Against Trojan Attacks on Deep Neural Networks (2019) [[paper](https://arxiv.org/pdf/1902.06531.pdf)]

* ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (CCS 2019) [[paper](https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf)]

* Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs (2019) [[paper](https://arxiv.org/pdf/1906.10842.pdf)]

* Detecting AI Trojans Using Meta Neural Analysis (2019) [[paper](https://arxiv.org/pdf/1910.03137.pdf)]

* Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification (2019) [[paper](https://arxiv.org/pdf/1908.10498.pdf)]

## Restoring Compromised Neural Models

### Model Correction

* Resilience of Pruned Neural Network Against Poisoning Attack (MALWARE 2018) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8659362)]

* Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks (ISRAID 2018) [[paper](https://arxiv.org/pdf/1805.12185.pdf)]

### Trigger-based Trojan Reversing

* Neural cleanse: Identifying and mitigating backdoor attacks in neural networks (2019) [[paper](http://people.cs.uchicago.edu/~huiyingli/publication/backdoor-sp19.pdf)]

* TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems (2019) [[paper](https://arxiv.org/pdf/1908.01763.pdf)]

* Detecting backdoor attacks on deep neural networks by activation clustering (2018) [[paper](https://arxiv.org/pdf/1811.03728.pdf)]

## Bypassing Neural Trojans

* Deep-Cleanse: A Black-box Input SanitizationFramework Against Backdoor Attacks on DeepNeural Networks (2019) [[paper](https://arxiv.org/pdf/1908.03369v1.pdf)]

* Neural trojans (ICCD, 2017) [[paper](https://arxiv.org/pdf/1710.00942.pdf)]
  
* Model Agnostic Defence against Backdoor Attacks in Machine Learning (2019) [[paper](https://arxiv.org/pdf/1908.02203.pdf)]
  
# Using Neural Trojans for Good

* Turning your weakness into a strength: Watermarking deep neural networks by backdooring (USENIX 2018) [[paper](https://arxiv.org/pdf/1802.04633.pdf)]

* Watermarking deep neural networks for embedded systems (ICCAD 2018) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8587745)]

* Using Honeypots to Catch Adversarial Attacks on Neural
Networks.[[paper](https://arxiv.org/pdf/1904.08554.pdf)]
